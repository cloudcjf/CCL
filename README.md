# Unsupervised-continual-place-recognition
## Problem definition
1. Joint training is expensive and sometimes impossible.
2. Massive data comes in sequence in the real world, we are unable to access previous data or all sequences at the same time.
3. When retraining a model on a different domain sequentially, catastrophic forgetting will be caused.
4. Labeling all the data is expensive and sometimes impossible.

## Baseline
**InCloud** has proposed a continual learning strategy for the sequential place recognition task. This strategy efficiently eliminates catastrophic forgetting. 

We plan to try unsupervised learning on unlabeled data in this sequential training process to avoid the expensive labeling process.

## Datasets
- Oxford
- Inhouse
- Mulran
- KITTI

## Place recognition methods
- MinkLoc3D
- Logg3d
- PointNetVLAD

## Experiments
### Step 1
Train the model on the Oxford dataset.

Use the pre-trained model to infer global descriptors on the Inhouse dataset.

Generate pseudo labels (query, positive, negative) according to the distances between descriptors.
Train on the new dataset with pseudo-labeled tuples.

### Results
**no_retrain**: Supervised training on Oxford dataset, evaluation on Inhouse dataset directly.

**retrain_wo_cl**: Supervised training on Oxford dataset, supervised retrain on Inhouse dataset, no continual learning strategy is implemented.

**retrain_wi_cl**: Supervised training on Oxford dataset, supervised retrain on Inhouse dataset, continual learning strategy is implemented.

**joint_train**: Supervised training on Oxford dataset and Inhouse dataset jointly.

**unsuper_retrain_wo_cl**: our method, Supervised training on Oxford dataset, unsupervised retrain on Inhouse dataset, no continual learning strategy is implemented.

**unsuper_retrain_wi_cl**: our method, Supervised training on Oxford dataset, unsupervised retrain on Inhouse dataset, continual learning strategy is implemented.

| Recall@1 | Oxford | Business | Resident | University |
|----|---|---|---|---|
| no_retrain | 93.8 | 82.7 | 81.1 | 86.0 |
| retrain_wo_cl | 70.3 | 93.3 | 96.4 | 96.3 |
| retrain_wi_cl | 90.7 | 93.3 | 95.8 | 96.1 |
| joint_train | 94.8 | 94.0 | 96.7 | 97.2 |
| unsuper_retrain_wo_cl | 73.2 | 86.0 | 87.2 | 87.8 |
| unsuper_retrain_wi_cl | 90.0 | 86.1 | 88.2 | 90.8 |

The results show that pseudo labels generated by the pre-trained model will reduce catastrophic forgetting (**unsuper_retrain_wo_cl VS retrain_wo_cl**).

The performance of the new dataset, however, needs to be improved.

### Step 2
Update the pseudo labels dynamically after every epoch.

Compare the unsupervised method with 10% labeling, 20% labeling, etc.

**undate_retrain_wo_cl**: Supervised training on Oxford dataset, unsupervised retrain on Inhouse dataset, update the labels after each epoch, continual learning strategy is not implemented.

**undate_retrain_wi_cl**: Supervised training on Oxford dataset, unsupervised retrain on Inhouse dataset, update the labels after each epoch, continual learning strategy is implemented.

**retrain_wo_cl_0.x**: Supervised training on Oxford dataset, supervised retrain on Inhouse dataset with part of the labeled data, no continual learning strategy is implemented.

**contrastive_wo_cl**: Supervised training on Oxford dataset, retrain on Inhouse dataset in contrastive settings, no continual learning strategy is implemented.

**contrastive_wi_cl**: Supervised training on Oxford dataset, retrain on Inhouse dataset in contrastive settings, continual learning strategy is implemented.

| Recall@1 | Oxford | Business | Resident | University |
|----|---|---|---|---|
| undate_retrain_wo_cl | 41.0 | 71.9 | 76.2 | 71.6 |
| undate_retrain_wi_cl | 88.4 | 88.8 | 91.0 | 92.8 |
| retrain_wo_cl_0.9 | 69.4 | 93.7 | 95.4 | 96.3 |
| retrain_wo_cl_0.75 | 67.8 | 93.0 | 94.9 | 95.6 |
| retrain_wo_cl_0.5 | 68.9 | 93.0 | 94.4 | 95.9 |
| retrain_wo_cl_0.25 | 65.8 | 89.7 | 90.7 | 92.4 |
| retrain_wo_cl_0.1 | 55.3 | 83.9 | 85.0 | 84.4 |
| retrain_wo_cl_0.01 | 27.3 | 35.3 | 36.4 | 38.5 |
| contrastive_wo_cl | 44.2 | 75.4 | 77.1 | 77.2 |
| contrastive_wi_cl | 61.2 | 80.8 | 88.6 | 86.2 |

![total loss](total_loss_60epochs.png)
![contrastive loss](contrastive_loss_60epochs.png)
![incremental loss](incremental_loss_60epochs.png)

The results show that training on the In-house dataset with contrastive learning is not useful to place recognition task.
It is even worse than directly testing on the new dataset with a pre-trained model on Oxford

### Step 3 2022-10-13
Forget about unsupervised training. We will use poses to divide positive and negative pairs.
1. Train with the contrastive learning strategy to improve the recall@1
2. contrastive continual learning to smoothly transfer among different datasets.

**Method**: MinkLoc3D
**oxford_triplet** : Training on the Oxford training pickle with triplet loss, evaluation on the Oxford and Inhouse evaluation pickles

**inhouse_triplet** : Training on the Inhouse training pickle with triplet loss, margin=0.2, evaluation on the Oxford and Inhouse evaluation pickles
**inhouse_triplet_0** : Training on the Inhouse training pickle with triplet loss, margin=0, evaluation on the Oxford and Inhouse evaluation pickles

**oxford_contrastive** : Training on the Oxford training pickle with contrastive loss, evaluation on the Oxford and Inhouse evaluation pickles

by temperature=0.04 batch=128, use projection head, loss following radar pr settings

**inhouse_contrastive** : Training on the Inhouse training pickle with contrastive loss, evaluation on the Oxford and Inhouse evaluation pickles
1.  by temperature=0.03 batch=128
2.  by temperature=0.04 batch=100, with regularization loss
3.  by temperature=0.04 batch=128, use projection head, loss following radar pr settings, but there is no big difference between loss version 1 and version 2, which means the prob of negatives maybe be useless

**incremental_contrastive** : Training on the Oxford training pickle with contrastive loss, then train on the Inhouse pickle with contrastive and incremental loss, evaluation on the Oxford and Inhouse evaluation pickles

| Recall@1 | Oxford | Business | Resident | University |
|----|---|---|---|---|
| oxford_triplet_provided_ckpt | 93.8 | 82.7 | 81.1 | 86.0 |
| oxford_triplet | 92.1 | 74.8 | 77.2 | 81.5 |
| oxford_contrastive | 86.5 | 78.9 | 78.3 | 83.5 |
| inhouse_triplet | 63.4 | 90.3 | 93.2 | 94.2 |
| inhouse_triplet_0 | 54.0 | 74.3 | 78.8 | 83.0 |
| inhouse_contrastive_1 | 47.4 | 89.1 | 91.6 | 89.1 |
| inhouse_contrastive_2 | 51.0 | 89.1 | 91.5 | 90.8 |
| inhouse_contrastive_3 | 62.0 | 89.0 | 91.9 | 92.7 |
| incremental_contrastive | 69.4 | 75.9 | 78.3 | 80.7 |